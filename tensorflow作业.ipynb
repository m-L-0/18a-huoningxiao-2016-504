{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.常量，变量，占位符\n",
    "#1.构建二元线性回归模型，其中模型中的参数w,b使用`tf.Variable()`构建，模型的样本输入x使用`tf.placeholder`代替\n",
    "import tensorflow as tf\n",
    "\n",
    "w=tf.Variable(10)\n",
    "b=tf.Variable(10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(w.initializer)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(b.initializer)\n",
    "    \n",
    "x = tf.placeholder(dtype=tf.int32, shape=[])#占位符必须包含数据类型!!!!!\n",
    "y=tf.multiply(w,x)+b\n",
    "#图构建好之后，运行图时，需要使用张量替代占位符，否则这个图无法运行\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y, feed_dict={x: 5}))\n",
    "    \n",
    "#使用tf.constant()生成常量时，不能使用其他常量张量初始化\n",
    "#各种常量类型包括普通常量、序列常量、随机数常量、特殊常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.名字与作用域\n",
    "#1. 总结`name_scope`与`variable_scope`的作用以及异同点\n",
    "\n",
    "#name_scope的作用：\n",
    "#1.解决功能近似的节点name可能命名重复的问题\n",
    "#2.为其作用域中的节点的name添加一个或多个前缀，并使用这些前缀作为划分内部与外部op范围的标记，同时在TensorFlow可\n",
    "    #视化时可以作为一个整体出现\n",
    "#3.name_scope可以通过划分操作范围来组织图结构，并能服务于可视化\n",
    "#4.name_scope可以给Op的name加前缀，但不包括tf.get_variable()创建的变量!!!!!!\n",
    "\n",
    "#variable_scope的作用：\n",
    "#1.主要用于管理变量作用域以及与变量相关的操作\n",
    "#2.variable_scope也可以像name_scope一样用来给不同操作区域划分范围（添加name前缀）\n",
    "#3.功能更为丰富，最重要的是可以与tf.get_variable()等配合使用完成对变量的重复使用\n",
    "\n",
    "#相同点：\n",
    "#1.都可以用来给不同操作区域划分范围（添加name前缀）\n",
    "#2.在运行时均可返回一个上下文管理器\n",
    "\n",
    "#不同点：\n",
    "#1.name_scope不能给tf.get_variable()创建的变量加前缀，而variabel_scope可以与tf.get_variable()等配合使用\n",
    "    #完成对变量的重复使用\n",
    "#2.variable_scope包含了name_scope的全部功能，即在variable_scope下也可以给Op与Tensor加上name_scope\n",
    "#3.变量作用域是可以嵌套使用的\n",
    "\n",
    "\n",
    "#2. 构建逻辑回归模型（只有模型部分，不包括训练部分），使用`get_variable`与`variable_scope`\n",
    "#将变量的创建与使用分开。提示：使用`tf.nn.sigmoid`实现`logistic`函数\n",
    "\n",
    "import tensorflow as tf\n",
    "#创建占位符\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    " \n",
    "#创建变量\n",
    "#tf.random_normal([1])返回一个符合正态分布的随机数\n",
    "with tf.variable_scope('var',reuse=None) as var:\n",
    "    tf.get_variable('w1',initializer=tf.random_normal([1],name='weight'))\n",
    "    tf.get_variable('b1',initializer=tf.random_normal([1],name='bias'))\n",
    "with tf.variable_scope('var',reuse=True) as var:\n",
    "    w = tf.get_variable('w1')\n",
    "    b = tf.get_variable('b1')\n",
    "\n",
    "y_predict=tf.sigmoid(tf.add(tf.multiply(X,w),b))\n",
    "num_samples=100\n",
    "cost=tf.reduce_sum(tf.pow(y_predict-Y,2.0))/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 3.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "#6.流程控制\n",
    "#1. 设计一个函数，要求输入两个`shape、dtype`一样的张量，输出一个同样`shape、dtype`的张量，并且输出的张量中的元素的每一个值都是输入的\n",
    "#两个张量中对应元素最大的。即模拟`tf.maximum`的功能，但不能直接使用此函数。\n",
    "import tensorflow as tf\n",
    "def my_maximum(x,y):\n",
    "    shape = x.get_shape()\n",
    "    maximum = tf.ones(shape=shape)\n",
    "    #with tf.Session() as sess:\n",
    "    #    rank = sess.run(tf.rank(x))\n",
    "    with tf.Session() as sess:\n",
    "        x = sess.run(x)\n",
    "        y = sess.run(y)\n",
    "        maximum = sess.run(maximum)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if x[i][j]<y[i][j]:\n",
    "                maximum[i][j]=y[i][j]\n",
    "            else:\n",
    "                maximum[i][j]=x[i][j]\n",
    "    return maximum\n",
    "x = tf.constant([[1,2],[3,4]])\n",
    "y = tf.constant([[1,3],[2,4]])\n",
    "maximum = my_maximum(x,y)\n",
    "print(maximum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 4]\n",
      "[3 5 4]\n"
     ]
    }
   ],
   "source": [
    "#2.了解`tf.minimum`、`tf.maximum`的用法\n",
    "\n",
    "#tf.maximum：用法tf.maximum(a,b),返回的是a,b之间的最大值\n",
    "#tf.miniimum：用法tf.miiinimum(a,b),返回的是a,b之间的最小值\n",
    "import tensorflow as tf\n",
    "a = [3,6,4] \n",
    "f1 = tf.maximum(a, 2) \n",
    "f2 = tf.minimum(a, 5) \n",
    "with tf.Session() as sess: \n",
    "    print(sess.run(f1))\n",
    "    print(sess.run(f2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[6.5 9.6]\n",
      " [6.5 9.6]\n",
      " [6.5 9.6]]\n"
     ]
    }
   ],
   "source": [
    "#3. 了解`TensorArray`的基本用法，并尝试配合`tf.while_loop`配合使用。\n",
    "import tensorflow as tf \n",
    "def cond(time, output_ta_l): \n",
    "    return tf.less(time, 3) \n",
    "def body(time, output_ta_l): \n",
    "    output_ta_l = output_ta_l.write(time, [6.5, 9.6]) \n",
    "    return time + 1, output_ta_l \n",
    "time = tf.constant(0) \n",
    "output_ta = tf.TensorArray(dtype=tf.float32, size=1, dynamic_size=True) \n",
    "result = tf.while_loop(cond, body, loop_vars=[time, output_ta]) \n",
    "last_time, last_out = result \n",
    "final_out = last_out.stack() \n",
    "with tf.Session(): \n",
    "    print(last_time.eval()) \n",
    "    print(final_out.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
